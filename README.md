# Qwen2-0.5B GaLore å…¨å‚æ•°å¾®è°ƒå®éªŒ

åœ¨ 4GB æ˜¾å­˜æé™ç¯å¢ƒä¸‹ï¼Œä½¿ç”¨ GaLoreï¼ˆGradient Low-Rank Projectionï¼‰ä¼˜åŒ–å™¨å®Œæˆ Qwen2-0.5B æ¨¡å‹çš„å…¨å‚æ•°å¾®è°ƒã€‚

## ğŸ¯ å®éªŒç›®æ ‡

éªŒè¯åœ¨æ¶ˆè´¹çº§ 4GB æ˜¾å¡ä¸Šï¼Œä½¿ç”¨ GaLore + 8-bit é‡åŒ– + Gradient Checkpointing çš„ç»„åˆï¼Œèƒ½å¦å®Œæˆ 0.5B å‚æ•°å¤§è¯­è¨€æ¨¡å‹çš„å…¨å‚æ•°å¾®è°ƒã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
Sophia&GaLore/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ galore_config.yaml      # å®éªŒé…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒä¸€é”®åˆ‡æ¢ä¼˜åŒ–å™¨ï¼‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py              # æ•°æ®é›†åŠ è½½ä¸å¤„ç†
â”‚   â””â”€â”€ sample_data.json        # ç¤ºä¾‹æŒ‡ä»¤æ•°æ®
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_loader.py         # æ¨¡å‹åŠ è½½ä¸ 8-bit é‡åŒ–
â”‚   â””â”€â”€ galore_hook.py          # GaLore å±‚çº§æ›´æ–° Hook æ ¸å¿ƒå®ç°
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ memory_monitor.py       # æ˜¾å­˜ç›‘æ§å·¥å…·
â”‚   â””â”€â”€ checkpoint.py           # æ–­ç‚¹ä¿å­˜ä¸æ¢å¤é€»è¾‘
â”œâ”€â”€ train.py                    # è®­ç»ƒä¸»å…¥å£
â”œâ”€â”€ download_model_modelscope.py # æ¨¡å‹ä¸‹è½½è„šæœ¬
â”œâ”€â”€ requirements.txt            # ä¾èµ–æ¸…å•
â””â”€â”€ README.md                   # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n galore python=3.10
conda activate galore

# å®‰è£… PyTorch (CUDA ç‰ˆæœ¬æ ¹æ®æ‚¨çš„æ˜¾å¡è°ƒæ•´)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt
```

### 2. ä¸‹è½½æ¨¡å‹

```bash
# ä½¿ç”¨ ModelScopeï¼ˆå›½å†…é•œåƒï¼Œé€Ÿåº¦å¿«ï¼‰
python download_model_modelscope.py
```

æ¨¡å‹å°†ä¸‹è½½åˆ° `./models_cache/qwen/Qwen2-0___5B-Instruct/`

### 3. å‡†å¤‡æ•°æ®

```bash
# ç”Ÿæˆç¤ºä¾‹æŒ‡ä»¤æ•°æ®ï¼ˆ1000 æ¡ï¼‰
python train.py --prepare-data
```

### 4. å¯åŠ¨è®­ç»ƒ

**GaLore ä¼˜åŒ–å™¨ï¼ˆæ˜¾å­˜ä¼˜åŒ–ï¼‰**
```bash
python train.py --optimizer galore
```

**AdamW ä¼˜åŒ–å™¨ï¼ˆåŸºçº¿å¯¹ç…§ï¼‰**
```bash
python train.py --optimizer adamw
```

**æ–­ç‚¹ç»­è®­**
```bash
python train.py --resume
```

## ğŸ”¬ å®éªŒè®¾è®¡

### æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯æ ˆ

| æŠ€æœ¯ | ä½œç”¨ | å®ç°ä½ç½® |
|------|------|---------|
| **8-bit é‡åŒ–** | æ¨¡å‹æƒé‡å‹ç¼© 50% | `models/model_loader.py` |
| **Gradient Checkpointing** | æ¿€æ´»å€¼é‡è®¡ç®—ï¼ŒèŠ‚çœ 30-40% æ˜¾å­˜ | `models/model_loader.py` |
| **GaLore** | æ¢¯åº¦ä½ç§©æŠ•å½±ï¼Œä¼˜åŒ–å™¨çŠ¶æ€å‹ç¼© | `models/galore_hook.py` |
| **Layer-wise Hook** | æ¢¯åº¦å³åˆ»æŠ•å½±å¹¶é‡Šæ”¾ | `models/galore_hook.py` |

### å…³é”®é…ç½®å‚æ•°

```yaml
# configs/galore_config.yaml
training:
  max_length: 512          # é™åˆ¶åºåˆ—é•¿åº¦
  batch_size: 1            # å¿…é¡»è®¾ä¸º 1
  gradient_accumulation_steps: 8  # æœ‰æ•ˆ batch = 8
  
quantization:
  enabled: true
  load_in_8bit: true       # 8-bit æ¨¡å‹æƒé‡

optimizer:
  type: "galore"           # ä¸€é”®åˆ‡æ¢: "galore" | "adamw"
  galore:
    rank: 128              # ä½ç§©æŠ•å½±ç»´åº¦
    update_proj_gap: 200   # æ¯ 200 æ­¥æ›´æ–°æŠ•å½±çŸ©é˜µ
    scale: 0.25
```

## ğŸ“Š å®éªŒç»“æœ

### è®­ç»ƒæ”¶æ•›æƒ…å†µ

| Epoch | GaLore Eval Loss | AdamW Eval Loss |
|-------|-----------------|-----------------|
| 0 | 0.200977 | 0.200977 |
| 1 | 0.010484 | 0.010484 |
| 2 | 0.004522 | 0.004522 |

**æ”¶æ•›æ›²çº¿å®Œå…¨ä¸€è‡´**ï¼ŒLoss ä» 0.20 é™åˆ° 0.0045ï¼Œé™ä½äº† **97.7%**ã€‚

### æ˜¾å­˜å ç”¨å¯¹æ¯”

| æŒ‡æ ‡ | GaLore | AdamW | ç»“è®º |
|------|--------|-------|------|
| å³°å€¼æ˜¾å­˜å ç”¨ | ~3.5 GB | ~3.5 GB | ç›¸åŒ |
| æ¨¡å‹åŠ è½½å | ~0.97 GB | ~0.97 GB | ç›¸åŒ |
| è®­ç»ƒé€Ÿåº¦ | ~8 min/epoch | ~8 min/epoch | ç›¸åŒ |

### å…³é”®å‘ç°

1. **ä¸¤è€…éƒ½èƒ½å®Œæˆè®­ç»ƒ**ï¼šåœ¨ 4GB æ˜¾å­˜é™åˆ¶ä¸‹ï¼ŒGaLore å’Œ 8-bit AdamW éƒ½æˆåŠŸå®Œæˆäº† 3 ä¸ª epoch çš„å…¨å‚æ•°å¾®è°ƒã€‚

2. **æ˜¾å­˜å ç”¨å‡ ä¹ç›¸åŒ**ï¼šåŸå› æ˜¯ï¼š
   - æ¨¡å‹è¾ƒå°ï¼ˆ0.5Bï¼‰ï¼Œä¼˜åŒ–å™¨çŠ¶æ€ä¸æ˜¯æ˜¾å­˜ç“¶é¢ˆ
   - 8-bit AdamW å·²ç»æŠŠä¼˜åŒ–å™¨çŠ¶æ€å‹ç¼©äº† 75%
   - æ¨¡å‹æƒé‡ï¼ˆ~0.5GBï¼‰å’Œæ¿€æ´»å€¼ï¼ˆ~2GBï¼‰å æ®äº†å¤§éƒ¨åˆ†æ˜¾å­˜

3. **GaLore çš„çœŸæ­£ä¼˜åŠ¿åœºæ™¯**ï¼š
   - æ›´å¤§æ¨¡å‹ï¼ˆ1B+ï¼‰ï¼šAdamW ä¼š OOMï¼ŒGaLore èƒ½è·‘
   - æ›´å¤§ batch_sizeï¼šGaLore çš„æ¢¯åº¦å‹ç¼©ä¼˜åŠ¿æ›´æ˜æ˜¾
   - æ›´é•¿åºåˆ—é•¿åº¦ï¼šæ¿€æ´»å€¼å ç”¨å¢åŠ ï¼Œä¼˜åŒ–å™¨çŠ¶æ€å æ¯”ç›¸å¯¹æé«˜

## ğŸ” æ ¸å¿ƒä»£ç è§£æ

### GaLore Layer-wise Hook æœºåˆ¶

```python
# models/galore_hook.py
class GaLoreProjector:
    def project(self, grad: torch.Tensor, step: int) -> torch.Tensor:
        # æ¯ N æ­¥ç”¨ SVD è®¡ç®—æŠ•å½±çŸ©é˜µ
        if step % self.update_proj_gap == 0:
            U, _, _ = torch.linalg.svd(grad)
            self.ortho_matrix = U[:, :rank]
        
        # æŠ•å½±åˆ°ä½ç§©ç©ºé—´: G_low = U_r^T @ G
        low_rank_grad = self.ortho_matrix.T @ grad
        return low_rank_grad

# æ³¨å†Œ backward hook
def make_hook(param_name: str, proj: GaLoreProjector):
    def hook(grad: torch.Tensor) -> torch.Tensor:
        # âš¡ æ ¸å¿ƒï¼šåœ¨ backward æ—¶å³åˆ»æŠ•å½±
        low_rank_grad = proj.project(grad, step)
        return low_rank_grad  # è¿”å›ä½ç§©æ¢¯åº¦ï¼ŒåŸå§‹æ¢¯åº¦è‡ªåŠ¨é‡Šæ”¾
    return hook
```

**åˆ›æ–°ç‚¹**ï¼šåœ¨ `backward()` å®Œæˆæ—¶å³åˆ»æŠ•å½±å¹¶é‡Šæ”¾å®Œæ•´æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ç­‰åˆ° `optimizer.step()`ã€‚

### æ–­ç‚¹ç»­è®­æœºåˆ¶

```python
# utils/checkpoint.py
# å®Œæ•´ä¿å­˜ï¼š
# - æ¨¡å‹æƒé‡ (safetensors)
# - ä¼˜åŒ–å™¨çŠ¶æ€ (å« 8-bit çŠ¶æ€)
# - GaLore æŠ•å½±çŸ©é˜µ (ortho_matrix)
# - éšæœºç§å­çŠ¶æ€ (ç¡®ä¿å¯å¤ç°)
```

## ğŸ› ï¸ å·¥ç¨‹ç‰¹æ€§

- **ä¸€é”®åˆ‡æ¢ä¼˜åŒ–å™¨**ï¼šä¿®æ”¹ `configs/galore_config.yaml` ä¸­çš„ `optimizer.type`
- **æ˜¾å­˜å®æ—¶ç›‘æ§**ï¼šæ¯ N æ­¥è¾“å‡º `torch.cuda.memory_reserved()` å’Œ loss
- **è‡ªåŠ¨æ–­ç‚¹ç»­è®­**ï¼šæ”¯æŒä»æœ€è¿‘ checkpoint æ¢å¤ï¼ŒåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºæ‰©å±•æ–°çš„æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯

## ğŸ“ˆ å®éªŒç»“è®º

1. âœ… **ç›®æ ‡è¾¾æˆ**ï¼šåœ¨ 4GB æ˜¾å­˜ä¸‹æˆåŠŸå®Œæˆ Qwen2-0.5B å…¨å‚æ•°å¾®è°ƒ
2. âš ï¸ **GaLore vs AdamW**ï¼šåœ¨ 0.5B æ¨¡å‹ä¸Šå·®å¼‚ä¸æ˜æ˜¾ï¼Œä¸¤è€…éƒ½å¯è¡Œ
3. ğŸ’¡ **å·¥ç¨‹ä»·å€¼**ï¼šéªŒè¯äº† 8-bit é‡åŒ– + Gradient Checkpointing æ˜¯ 4GB æ˜¾å­˜å¾®è°ƒçš„åº•çº¿æ–¹æ¡ˆ
4. ğŸ”® **æœªæ¥æ–¹å‘**ï¼šGaLore çš„çœŸæ­£ä¼˜åŠ¿åœ¨æ›´å¤§æ¨¡å‹ï¼ˆ1B+ï¼‰ä¸Šä¼šæ˜¾ç°

## ğŸ“š å‚è€ƒèµ„æ–™

- [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507)
- [bitsandbytes: 8-bit Optimizers](https://github.com/TimDettmers/bitsandbytes)
- [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)

## ğŸ“ License

MIT License
